{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":11359,"sourceType":"modelInstanceVersion","modelInstanceId":8749}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U immutabledict sentencepiece \n!git clone https://github.com/google/gemma_pytorch.git\n!mkdir /kaggle/working/gemma/\n!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/\n\nimport sys \nsys.path.append(\"/kaggle/working/gemma_pytorch/\") \nfrom gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport contextlib\nimport os\nimport torch\n\n# Load the model\nVARIANT = \"7b-it-quant\" \nMACHINE_TYPE = \"cuda\" \nweights_dir = '/kaggle/input/gemma/pytorch/7b-it-quant/2' \n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n# Model Config.\nmodel_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\nmodel_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\nmodel_config.quant = \"quant\" in VARIANT\n\n# Model.\ndevice = torch.device(MACHINE_TYPE)\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)\n    ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n    model.load_weights(ckpt_path)\n    model = model.to(device).eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:22:57.967702Z","iopub.execute_input":"2024-03-14T16:22:57.968438Z","iopub.status.idle":"2024-03-14T16:24:21.222309Z","shell.execute_reply.started":"2024-03-14T16:22:57.968409Z","shell.execute_reply":"2024-03-14T16:24:21.221198Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"fatal: destination path 'gemma_pytorch' already exists and is not an empty directory.\nmkdir: cannot create directory '/kaggle/working/gemma/': File exists\nmv: cannot stat '/kaggle/working/gemma_pytorch/gemma/*': No such file or directory\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:41:17.143813Z","iopub.execute_input":"2024-03-14T16:41:17.144448Z","iopub.status.idle":"2024-03-14T16:41:17.148893Z","shell.execute_reply.started":"2024-03-14T16:41:17.144418Z","shell.execute_reply":"2024-03-14T16:41:17.147832Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:32:45.032176Z","iopub.execute_input":"2024-03-14T16:32:45.032569Z","iopub.status.idle":"2024-03-14T16:32:45.043914Z","shell.execute_reply.started":"2024-03-14T16:32:45.032538Z","shell.execute_reply":"2024-03-14T16:32:45.043082Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"template = \"\"\"Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. \nYour task is to carefully analyze the differences between the `Original Text` and `Rewritten Text`, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.\n\nOriginal Text:\\n{original_text}\n\nRewriten Text:\\n{rewritten_text}\n\nReturn only the text used as instruction limited to one line.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:49:30.940805Z","iopub.execute_input":"2024-03-14T16:49:30.941711Z","iopub.status.idle":"2024-03-14T16:49:30.946006Z","shell.execute_reply.started":"2024-03-14T16:49:30.941676Z","shell.execute_reply":"2024-03-14T16:49:30.945062Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n\npreds = []\nfor i in tqdm(range(len(test))):\n    row = test.iloc[i]\n    \n    prompt = template.format(\n        original_text=row.original_text,\n        rewritten_text=row.rewritten_text\n    )\n\n    output = model.generate(USER_CHAT_TEMPLATE.format(prompt=prompt),\n                            device=device,\n                            output_len=100)\n    pred = output.replace(prompt, \"\")\n    \n    preds.append([row.id, pred])","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:49:32.967178Z","iopub.execute_input":"2024-03-14T16:49:32.967529Z","iopub.status.idle":"2024-03-14T16:50:15.279271Z","shell.execute_reply.started":"2024-03-14T16:49:32.967502Z","shell.execute_reply":"2024-03-14T16:50:15.278308Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:42<?, ?it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"sub_df = pd.DataFrame(preds, columns=[\"id\", \"rewrite_prompt\"])\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].fillna(\"\")\nsub_df['rewrite_prompt'] = sub_df['rewrite_prompt'].map(lambda x: \"Improve the essay\" if len(x) == 0 else x)\nsub_df.to_csv(\"submission.csv\",index=False)\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-14T16:50:19.717687Z","iopub.execute_input":"2024-03-14T16:50:19.718580Z","iopub.status.idle":"2024-03-14T16:50:19.733395Z","shell.execute_reply.started":"2024-03-14T16:50:19.718543Z","shell.execute_reply":"2024-03-14T16:50:19.732426Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"   id                                     rewrite_prompt\n0  -1  The prompt given to the LLM to rewrite the tex...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rewrite_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>The prompt given to the LLM to rewrite the tex...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}